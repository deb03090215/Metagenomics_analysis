# Metagenomics_pipeline
![workflow](Figures/pipeline.png)

## Input data

This pipeline is designed primarily for **paired-end sequencing data**, which is commonly generated by Illumina platforms.

Paired-end reads (R1 and R2) provide sequence information from both ends of a DNA/RNA fragment, improving:
- read mapping accuracy,
- de novo assembly quality,
- and downstream taxonomic annotation.

Single-end data can be adapted to this pipeline with minor modifications, but all examples below assume paired-end input.

## Step 1. Read preprocessing and quality control

Raw paired-end sequencing reads often contain adapters, low-quality bases, and technical artifacts.

In this pipeline, we perform:
##### Quality assessment with FastQC
```
fastqc raw_reads_R1.fastq.gz raw_reads_R2.fastq.gz

# (or run fast qc for several files at once)
fastqc *.fastq
```

```
multiqc .
```

##### Adapter and quality trimming with Trimmomatic (paired-end mode)
```
trimmomatic PE \
  raw_reads_R1.fastq.gz raw_reads_R2.fastq.gz \
  cleaned_R1_paired.fastq.gz cleaned_R1_unpaired.fastq.gz \
  cleaned_R2_paired.fastq.gz cleaned_R2_unpaired.fastq.gz \
  ILLUMINACLIP:adapters.fa:2:30:10 \
  LEADING:20 TRAILING:20 SLIDINGWINDOW:4:20 MINLEN:50
```

## Step 2. Host genome removal

In metagenomic sequencing data, a large proportion of reads often originate from the host genome.
To reduce background noise and improve downstream viral discovery, host-derived reads are removed prior to assembly.

In this pipeline, cleaned paired-end reads are mapped to the host reference genome using **Bowtie2**, and only **unmapped read pairs** are retained for subsequent analysis.

```
bowtie2 -x host_index \
  -1 cleaned_R1_paired.fastq.gz \
  -2 cleaned_R2_paired.fastq.gz \
  --very-sensitive \
  --un-conc-gz non_host_reads.fastq.gz \
  -S host_mapped.sam
```

This step generates paired-end non-host reads (`non_host_reads.1.fastq.gz` and `non_host_reads.2.fastq.gz`), which are used as input for de novo assembly.

## Step 3. De novo assembly

Non-host paired-end reads are assembled into contigs using **de novo assembly**, which enables the reconstruction of longer genomic fragments from short sequencing reads.
Assembly is a critical step for downstream sequence similarity searches and viral identification.

In this pipeline, we evaluated multiple assemblers, including **MEGAHIT**, **SPAdes**, and **rnaSPAdes**, and ultimately selected **SPAdes** based on overall assembly quality and contig characteristics.

### Assembly with SPAdes

```bash
spades.py \
  -1 non_host_reads.1.fastq.gz \
  -2 non_host_reads.2.fastq.gz \
  -o spades_out
```

The resulting contigs (`contigs.fasta`) are used for downstream BLAST and DIAMOND analyses.

#### Notes on assembler selection

During preliminary analyses, we compared assembly statistics across different assemblers.
While **SPAdes** produced contigs with better overall length distribution and assembly metrics for our dataset, **MEGAHIT** remains a strong alternative for large-scale studies involving a very large number of samples, due to its lower memory usage and faster runtime.

In practice:

* **SPAdes** is well suited for in-depth analysis of individual or moderate-sized datasets, where assembly quality is prioritized.
* **MEGAHIT** is more appropriate for high-throughput metagenomic studies with hundreds or thousands of samples.

Users may choose the assembler that best fits their dataset size and computational resources.

## Step 4. Database search (Sequence similarity search)

To assign putative taxonomic information to assembled contigs, sequence similarity searches are performed against reference databases.

In this pipeline, we apply **both nucleotide-level and protein-level searches** to improve viral detection sensitivity:

* **BLASTn** is used to identify contigs with high nucleotide similarity to known viral genomes.
* **DIAMOND (blastx)** is used to detect more divergent viral sequences by translating nucleotide sequences into proteins and searching against protein databases.

Using both approaches allows the identification of viruses across different evolutionary distances.

### Nucleotide-level search with BLASTn

```bash
blastn \
  -query contigs.fasta \
  -db nt \
  -out blast.tsv \
  -evalue 1e-5 \
  -max_target_seqs 10 \
  -outfmt "6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore sskingdoms ppos frames qframe sframe staxids stitle"
```

### Protein-level search with DIAMOND

```bash
diamond blastx \
  -q contigs.fasta \
  -d nr \
  -o diamond.tsv \
  -e 1e-5 \
  -k 10 \
  --outfmt 6 \
  qseqid sseqid pident length mismatch gapopen qstart qend qlen \
  sstart send slen evalue bitscore staxids sscinames sskingdoms skingdoms sphylums
```

The resulting tabular outputs (`blast.tsv` and `diamond.tsv`) are subsequently processed and filtered to extract viral-related hits and resolve discrepancies between nucleotide- and protein-based annotations.

## Step 5. Parsing and filtering (viral candidate extraction)

Raw similarity search results often include low-quality hits, redundant matches, and non-viral annotations.
To obtain a clean set of viral candidates for downstream interpretation, we apply a standardized parsing and filtering procedure to both BLAST and DIAMOND outputs.

### Inputs

* `blast.tsv` (from Step 4; BLASTn output with custom fields)
* `diamond.tsv` (from Step 4; DIAMOND output with custom fields)

### What this step does

* **Deduplicate** redundant hits (e.g., repeated matches for the same query/subject)
* **Filter by alignment quality** (e.g., bitscore / e-value / alignment length; thresholds can be adjusted)
* **Extract virus-associated hits** based on taxonomy fields (e.g., `sskingdoms`, `skingdoms`, `sphylums`, `stitle`)
* Generate **clean summary tables** for downstream interpretation and visualization

### Run parsing scripts

BLAST parsing:

```bash
python filter_blast.py --input BLAST.tsv
```

DIAMOND parsing:

```bash
python filter_diamond.py --input diamond.tsv
```

### Outputs

* `BLAST_parsing.csv`
* `diamond_parsing.csv`

### Notes on taxonomy-based filtering

In this pipeline, a **virome dataset** is used as the primary example.
Therefore, during the parsing and filtering step, taxonomy-based filtering focuses on entries annotated with **`Superkingdom = Viruses`**, allowing enrichment of virus-associated hits while removing non-viral sequences.

This criterion is **dataset-dependent** and can be modified according to the biological question of interest.
For example, when analyzing datasets targeting other organism groups (e.g., bacteria, archaea, or eukaryotes), the taxonomy filtering rules can be adjusted accordingly by changing the target taxonomic category in the parsing scripts.

Users are encouraged to customize taxonomy filters to match their specific study design and sample type.


## Step 6. Candidate selection

Both BLASTn and DIAMOND provide complementary information for viral identification, but their results may not always fully overlap.
To obtain a comprehensive view of viral candidates, the parsed outputs from Step 5 are compared and jointly interpreted.

### Rationale

* **BLASTn** is more reliable for detecting closely related viruses with high nucleotide similarity.
* **DIAMOND (blastx)** is more sensitive for identifying divergent or novel viruses through protein-level conservation.

As a result:

* Some viral candidates may be detected by **both** methods.
* Others may be detected **only by DIAMOND**, especially in the case of highly divergent viruses.
* BLASTn-only hits may represent conserved genomic regions or closely related reference strains.

### Inputs

* `BLAST_parsing.csv`
* `diamond_parsing.csv`

### Run scripts

BLAST:

```
python process_blast_0629.py \
  BLAST_parsing.csv \
  non_host_reads.1.fastq.gz \
  non_host_reads.1.fastq.gz
```

DIAMOND:

```
python process_diamond_hmmscan_0714.py \
  diamond_parsed.csv \
  contigs.fasta \
  non_host_reads.1.fastq.gz \
  non_host_reads.1.fastq.gz
```

### Outputs

* `{Accession_number}_aln.sorted.bam` & `{Accession number}_aln.sorted.bam.bai`
* `mapping_matrix.csv`

## (Optional) Step 7. Construction of consensus sequences (BLAST workflow only)

This step is **only required for the BLAST-based workflow**.

In Step 6 (BLAST), reads are mapped to reference sequences automatically downloaded from NCBI.
These reference FASTA files are stored under the `NCBI_nt/` directory and are named using the accession ID (e.g., `BK016511.1.fasta`).

A consensus sequence is generated from the **sorted and indexed BAM file** using **Samtools**.

In contrast, the **DIAMOND workflow does not require an additional consensus-building step**.
For DIAMOND, reads are mapped back to `contigs.fasta` in Step 6 to calculate **depth and coverage**, where the contigs themselves already serve as the sequence template. Therefore, no separate consensus sequence is constructed.

### Input

* `{Accession_ID}_aln.sorted.bam` and `{Accession_ID}_aln.sorted.bam.bai`
  (generated in Step 6; BLAST workflow)
* `NCBI_nt/{Accession_ID}.fasta`
  (reference sequence downloaded automatically in Step 6)

### Generate consensus using Samtools

```bash
samtools consensus \
  -f NCBI_nt/{Accession_ID}.fasta \
  {Accession_ID}_aln.sorted.bam \
  > {Accession_ID}_consensus.fasta
```

### Output

* `{Accession_ID}_consensus.fasta`


